{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM for word embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4808字\n",
    "with open('./data/words.pickle', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_init(n, d=300):\n",
    "    wordv = torch.zeros(1, d, dtype=torch.float)\n",
    "    n0 = n // d\n",
    "    n1 = n % d\n",
    "    for i in range(d):\n",
    "        if n1 > i:\n",
    "            wordv[0, i] = n0 + 1\n",
    "        else:\n",
    "            wordv[0, i] = n0\n",
    "        \n",
    "    wordv = F.softmax(wordv) * 1000\n",
    "    return wordv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_init = {}\n",
    "m = 200\n",
    "for i, w in enumerate(words):\n",
    "    words_init[w] = torch.rand(1, m, dtype=torch.float) * 5\n",
    "words_init['other'] = torch.rand(1, m, dtype=torch.float) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, words_init, m):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.d = m // 2      #字轉出來後的向量長度，理論上為 m 的一半\n",
    "        self.m = m        #字的embedding長度\n",
    "        self.word_v = words_init\n",
    "        \n",
    "        #正向層 代號 p\n",
    "        self.g = torch.rand(self.d, self.m, requires_grad=True, dtype=torch.float)  #輸入層\n",
    "        self.h = torch.rand(self.m, self.d, requires_grad=True, dtype=torch.float)  #輸出層\n",
    "        self.f1 = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #input gate\n",
    "        self.f2 = torch.rand(1, self.d, requires_grad=True, dtype=torch.float)  #forget gate\n",
    "        self.f3 = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #output gate\n",
    "        \n",
    "        self.gb = torch.rand(1, self.d, requires_grad=True, dtype=torch.float)  #輸入層bias\n",
    "        self.hb = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #輸出層bias\n",
    "        \n",
    "        #反向層 代號 r\n",
    "        self.gr = torch.rand(self.d, self.m, requires_grad=True, dtype=torch.float)  #輸入層\n",
    "        self.hr = torch.rand(self.m, self.d, requires_grad=True, dtype=torch.float)  #輸出層\n",
    "        self.f1r = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #input gate\n",
    "        self.f2r = torch.rand(1, self.d, requires_grad=True, dtype=torch.float)  #forget gate\n",
    "        self.f3r = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #output gate\n",
    "        \n",
    "        self.gbr = torch.rand(1, self.d, requires_grad=True, dtype=torch.float)  #輸入層bias\n",
    "        self.hbr = torch.rand(1, self.m, requires_grad=True, dtype=torch.float)  #輸出層bias\n",
    "        \n",
    "        \n",
    "        self.allparameters = [self.g, self.h, self.f1, self.f2, self.f3, self.gb, self.hb,\n",
    "                              self.gr, self.hr, self.f1r, self.f2r, self.f3r, self.gbr, self.hbr]\n",
    "        \n",
    "    #sent 代表一句話, dirct 代表正向或反向\n",
    "    def forward(self, sent, dirct):\n",
    "        memory = torch.zeros(1, self.d, dtype=torch.float)      # memory cell\n",
    "        output = []\n",
    "        \n",
    "        if dirct == 'p':\n",
    "            for i, w in enumerate(sent):\n",
    "                #print(w)\n",
    "                wv = self.word2em(w)       #轉對應向量\n",
    "                \n",
    "                #計算 Gate 值\n",
    "                wv_f1 = self.sigmoid(self.f1.mm(wv.t()), 1)  # input Gate\n",
    "                wv_f3 = self.sigmoid(self.f3.mm(wv.t()), 1)  #output Gate\n",
    "                memory_f2 = self.sigmoid(memory.mm(self.f2.t()), 1)       #forget Gate   並經過\n",
    "                \n",
    "                #輸入層\n",
    "                wv_g = (self.g.mm(wv.t()) + self.gb.t())\n",
    "                wv_g = self.softmax(wv_g, 500)\n",
    "                self.wordv_update(w, wv_g, 'p')     #更新字向量\n",
    "                \n",
    "                #途中\n",
    "                wv_g = wv_g * wv_f1                 #輸入經過門\n",
    "                wv_cross = wv_g + (memory * memory_f2).t()\n",
    "                wv_cross = wv_cross * wv_f3\n",
    "                \n",
    "                #輸出層\n",
    "                wv_h = self.h.mm(wv_cross) + self.hb.t()\n",
    "                wv_output = wv_h * wv_f3\n",
    "                wv_output = self.softmax(wv_output, 500)\n",
    "                \n",
    "                #更新 memory\n",
    "                cc = memory + wv_g.t()\n",
    "                memory = cc.detach().clone()\n",
    "                \n",
    "                #append output\n",
    "                output.append(wv_output.t())\n",
    "                \n",
    "        if dirct == 'r':\n",
    "            for i, w in enumerate(sent):\n",
    "                wv = self.word2em(w)       #轉對應向量\n",
    "                \n",
    "                #計算 Gate 值\n",
    "                wv_f1 = self.sigmoid(self.f1r.mm(wv.t()), 1)  # input Gate\n",
    "                wv_f3 = self.sigmoid(self.f3r.mm(wv.t()), 1)  #output Gate\n",
    "                memory_f2 = self.sigmoid(memory.mm(self.f2r.t()), 1)       #forget Gate   並經過\n",
    "                \n",
    "                #輸入層\n",
    "                wv_g = (self.gr.mm(wv.t()) + self.gbr.t())\n",
    "                wv_g = self.softmax(wv_g, 500)\n",
    "                self.wordv_update(w, wv_g, 'r')     #更新字向量\n",
    "                \n",
    "                #途中\n",
    "                wv_g = wv_g * wv_f1                 #輸入經過門\n",
    "                wv_cross = wv_g + (memory * memory_f2).t()\n",
    "                wv_cross = wv_cross * wv_f3\n",
    "                \n",
    "                #輸出層\n",
    "                wv_h = self.hr.mm(wv_cross) + self.hbr.t()\n",
    "                wv_output = wv_h * wv_f3\n",
    "                wv_output = self.softmax(wv_output, 500)\n",
    "                \n",
    "                #更新 memory\n",
    "                cc = memory + wv_g.t()\n",
    "                memory = cc.detach().clone()\n",
    "                \n",
    "                #append output\n",
    "                output.append(wv_output.t())\n",
    "            \n",
    "        return output\n",
    "            \n",
    "            \n",
    "    def softmax(self, input, p):\n",
    "        return F.softmax(input, dim=0) * p\n",
    "            \n",
    "    def sigmoid(self, input, p):\n",
    "        y = 1 / (1 + torch.exp(-p * input))\n",
    "        return y\n",
    "    \n",
    "    #更改字的向量, dirct 代表方向\n",
    "    def wordv_update(self, w, v, dirct):\n",
    "        cc = v.detach()\n",
    "        cc = cc.clone()\n",
    "#         print('cc:', cc.size())\n",
    "#         print('w:', self.word_v[w].size())\n",
    "        if dirct == 'p':\n",
    "            try:\n",
    "                v0 = torch.cat([cc.t(), self.word_v[w][:, :self.d]], 1)\n",
    "                self.word_v[w] = v0\n",
    "            except:\n",
    "                v0 = torch.cat([cc.t(), self.word_v['other'][:, :self.d]], 1)\n",
    "                self.word_v['other'] = v0\n",
    "            \n",
    "        if dirct == 'r':\n",
    "            try:\n",
    "                v0 = torch.cat([self.word_v[w][:, self.d:], cc.t()], 1)\n",
    "                self.word_v[w] = v0\n",
    "            except:\n",
    "                v0 = torch.cat([self.word_v['other'][:, self.d:], cc.t()], 1)\n",
    "                self.word_v['other'] = v0\n",
    "    \n",
    "    #將字轉成對應的向量\n",
    "    def word2em(self, w):\n",
    "        for i, j in self.word_v.items():\n",
    "            if w == i:\n",
    "                #print('i:', i)\n",
    "                return j\n",
    "            \n",
    "        return self.word_v['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_fn(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super(loss_fn, self).__init__()\n",
    "        self.m = m          #字的維度\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        n = len(pred)\n",
    "        output = 0\n",
    "        for i in range(n):\n",
    "            output += (pred[i] - target[i]).norm() / self.m\n",
    "        output = output / n\n",
    "            \n",
    "        return output    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2em(words_v, w):\n",
    "    for i, j in words_v.items():\n",
    "        if w == i:\n",
    "            #print('i:', i)\n",
    "            return j\n",
    "            \n",
    "    return words_v['other']\n",
    "\n",
    "#data是一筆資料包含很多句話\n",
    "def learn(model, lossf, opti, data, words_v):\n",
    "    for i in data:\n",
    "        #執行正向傳送一句話\n",
    "        target = []\n",
    "        for j in range(len(i)-1):\n",
    "            v = word2em(words_v=words_v, w=i[j+1])\n",
    "            target.append(v)\n",
    "        pred = model(i[:len(i)-1], 'p')\n",
    "        loss = lossf(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(model.g.grad)\n",
    "        optimizer.step()\n",
    "        #逆向傳送一句話\n",
    "        target = []\n",
    "        for j in range(len(i)-1):\n",
    "            v = word2em(words_v=words_v, w=i[len(i)-2-j])\n",
    "            target.append(v)\n",
    "        pred = model(i[len(i)-1:0:-1], 'r')\n",
    "        loss = lossf(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_model(words_init, 200)\n",
    "lossf = loss_fn(200)\n",
    "optimizer = torch.optim.SGD(model.allparameters, lr=10. ** 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = {}\n",
    "for i in range(10):\n",
    "    data0[i] = data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練第0筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第1筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第2筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第3筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第4筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第5筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第6筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第7筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第8筆資料-------------\n",
      "tensor(0.)\n",
      "訓練第9筆資料-------------\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for i, j in data0.items():\n",
    "    print('訓練第{}筆資料-------------'.format(i))\n",
    "    learn(model, lossf, optimizer, j[1], words_init)\n",
    "    print(words_init['你'][0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#拿一句話做測試\n",
    "s = data[0][1][0]\n",
    "target = []\n",
    "for i in range(len(s)-1):\n",
    "    v = word2em(words_v=words_init, w=s[i+1])\n",
    "    target.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(s[:len(s)-1], 'p')\n",
    "loss = lossf(pred, target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4704, 0.4377, 0.5744,  ..., 0.0810, 0.5575, 0.0334],\n",
       "        [0.9348, 0.7560, 0.7320,  ..., 0.7151, 0.5783, 0.1804],\n",
       "        [0.7975, 0.4105, 0.3010,  ..., 0.2504, 0.8116, 0.5864],\n",
       "        ...,\n",
       "        [0.4000, 0.7198, 0.7404,  ..., 0.4538, 0.8620, 0.1443],\n",
       "        [0.5054, 0.8693, 0.7465,  ..., 0.0606, 0.0556, 0.3115],\n",
       "        [0.6812, 0.7539, 0.3230,  ..., 0.6258, 0.5157, 0.3349]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6654, 0.3230, 0.7435,  ..., 0.0211, 0.6887, 0.5959],\n",
       "        [0.6095, 0.5095, 0.9073,  ..., 0.6551, 0.1527, 0.8466],\n",
       "        [0.5304, 0.2073, 0.2358,  ..., 0.7933, 0.2240, 0.3650],\n",
       "        ...,\n",
       "        [0.7761, 0.1615, 0.3112,  ..., 0.6138, 0.9309, 0.8598],\n",
       "        [0.1607, 0.7035, 0.7084,  ..., 0.5113, 0.1231, 0.1603],\n",
       "        [0.2839, 0.9830, 0.1132,  ..., 0.5589, 0.2199, 0.8213]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.g.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.f1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0., 500.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0., 500.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_init['你']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(s[:len(s)-1], 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = 0\n",
    "for i in o:\n",
    "    o1 += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = o1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.f3.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5986, 0.0635, 0.9246,  ..., 0.1328, 0.2470, 0.6777],\n",
       "        [0.2593, 0.5368, 0.3052,  ..., 0.6233, 0.2045, 0.2661],\n",
       "        [0.0082, 0.3588, 0.6924,  ..., 0.8754, 0.3814, 0.5536],\n",
       "        ...,\n",
       "        [0.0047, 0.2826, 0.4845,  ..., 0.9600, 0.9771, 0.7938],\n",
       "        [0.0032, 0.8949, 0.6886,  ..., 0.9581, 0.9224, 0.1015],\n",
       "        [0.8074, 0.8319, 0.3483,  ..., 0.3254, 0.6752, 0.0811]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lossf(o, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5952, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,5, requires_grad=True, dtype=torch.float)\n",
    "y = torch.randn(5, 5, requires_grad=True, dtype=torch.float)\n",
    "z = torch.tensor([[5, 6, 7, 8, 9]], dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
