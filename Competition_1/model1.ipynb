{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple RNN modle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/words.pickle', 'rb') as f:\n",
    "    words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_init = {}\n",
    "for i, j in enumerate(words):\n",
    "    word_vec_init[j] = torch.zeros(4809, 1, dtype=torch.float)\n",
    "    word_vec_init[j][i, 0] = 1.\n",
    "\n",
    "word_vec_init['other'] = torch.zeros(4809, 1, dtype=torch.float)\n",
    "word_vec_init[j][4808, 0] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_1(nn.Module):\n",
    "    def __init__(self, words_vec):\n",
    "        super(RNN_1, self).__init__()\n",
    "        self.words_vec = words_vec\n",
    "        self.matrix1 = torch.rand(4809, 4809, requires_grad=True, dtype=torch.float)\n",
    "        self.matrix2 = torch.rand(4809, 4809, requires_grad=True, dtype=torch.float)\n",
    "        self.matrix3 = torch.rand(4809, 4809, requires_grad=True, dtype=torch.float)\n",
    "        self.b1 = torch.zeros(4809, 1, requires_grad=True, dtype=torch.float)\n",
    "        self.b2 = torch.zeros(4809, 1, requires_grad=True, dtype=torch.float)\n",
    "        self.b3 = torch.zeros(4809, 1, requires_grad=True, dtype=torch.float)\n",
    "        \n",
    "        self.allparameters = [self.matrix1, self.matrix2, self.matrix3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "    def forward(self, x, m1=None, m2=None):\n",
    "        for i, j in self.words_vec.items():\n",
    "            if x == i:\n",
    "                x0 = j\n",
    "                break\n",
    "            \n",
    "            if i == 'other':\n",
    "                #print('這個是other')\n",
    "                x0 = j\n",
    "        \n",
    "        t = 0\n",
    "        if t == 0:\n",
    "            t = 1\n",
    "            y1 = (self.matrix1.mm(x0) + self.b1) / 1000\n",
    "            y2 = (self.matrix2.mm(y1) + self.b2) / 1000\n",
    "            y3 = (self.matrix3.mm(y2) +  self.b3) / 1000\n",
    "        \n",
    "        else:\n",
    "            if m1 & m2:\n",
    "                y1 = (self.matrix1.mm(x0) + self.b1 + m1) / 1000\n",
    "                y2 = (self.matrix2.mm(y1) + self.b2 + m2) / 1000\n",
    "                y3 = (self.matrix3.mm(y2) +  self.b3) / 1000\n",
    "                \n",
    "            else:\n",
    "                y1 = (self.matrix1.mm(x0) + self.b1) / 1000\n",
    "                y2 = (self.matrix2.mm(y1) + self.b2) / 1000\n",
    "                y3 = (self.matrix3.mm(y2) + self.b3) / 1000\n",
    "        cc = ((y1 + y2) / 2).detach()\n",
    "        self.words_vec[x] = cc.clone()\n",
    "        del(cc)\n",
    "        \n",
    "        return y1, y2, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_fn(nn.Module):\n",
    "    def __init__(self, words_vec):\n",
    "        super(loss_fn, self).__init__()\n",
    "        self.words_vec = words_vec\n",
    "        \n",
    "    def forward(self, pred, target_word=None):\n",
    "        if target_word == None:\n",
    "            return None\n",
    "        \n",
    "        for i, j in self.words_vec.items():\n",
    "            if target_word == i:\n",
    "                target = j\n",
    "                break\n",
    "            \n",
    "            if i == 'other':\n",
    "                #print('這個是other')\n",
    "                target = j\n",
    "        \n",
    "        loss = torch.zeros(4809, 1, dtype=torch.float)\n",
    "        for i in range(pred.size()[0]):\n",
    "            loss[i, 0] = pred[i, 0] - target[i, 0]\n",
    "        loss = loss.norm(p=2, dim=0, keepdim=True)    \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, model, loss_func, optimizer):\n",
    "    for i1, i2 in enumerate(data):\n",
    "        print('第  {}  筆資料----------------------'.format(i1))\n",
    "        for j1, j2 in enumerate(i2):\n",
    "            print('第  {}  句話'.format(j1))\n",
    "            m1 = None\n",
    "            m2 = None\n",
    "            for k1, k2 in enumerate(j2):\n",
    "                #print('第  {}  個字'.format(k1))\n",
    "                m1, m2, m3 = model(k2, m1=m1, m2=m2)\n",
    "                if k1 < len(j2)-1:\n",
    "                    try:\n",
    "                        loss = loss_func(m3, j2[k1+1])\n",
    "                        time.sleep(1)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    except:\n",
    "                        print('錯誤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN_1(word_vec_init)\n",
    "loss_func = loss_fn(word_vec_init)\n",
    "optimizer = torch.optim.SGD(rnn.allparameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = []\n",
    "for i, j in data.items():\n",
    "    data0.append(j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  0  筆資料----------------------\n",
      "第  0  句話\n",
      "第  1  句話\n",
      "第  2  句話\n",
      "第  3  句話\n",
      "第  4  句話\n",
      "錯誤\n",
      "第  5  句話\n",
      "第  6  句話\n",
      "錯誤\n",
      "錯誤\n",
      "錯誤\n",
      "錯誤\n",
      "第  7  句話\n"
     ]
    }
   ],
   "source": [
    "learn(data0, rnn, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你有做超音波嘛，那我們來看報告，有些部分有紅字耶。'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6585, 0.6632, 0.0649,  ..., 0.5977, 0.4874, 0.2572],\n",
       "        [0.3243, 0.1278, 0.0160,  ..., 0.3772, 0.4391, 0.1952],\n",
       "        [0.8113, 0.2271, 0.0245,  ..., 0.6491, 0.9799, 0.3763],\n",
       "        ...,\n",
       "        [0.8314, 0.9963, 0.6432,  ..., 0.3259, 0.0041, 0.5155],\n",
       "        [0.4378, 0.1108, 0.4247,  ..., 0.4296, 0.6539, 0.9203],\n",
       "        [0.6746, 0.9344, 0.3977,  ..., 0.8595, 0.4479, 0.7637]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.b2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_init['報']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你有做超音波嘛，那我們來看報告，有些部分有紅字耶。'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = data[0][1][0]\n",
    "m1 = m2 = None\n",
    "i = 0\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0008],\n",
       "        [0.0006],\n",
       "        [0.0009],\n",
       "        ...,\n",
       "        [0.0007],\n",
       "        [0.0007],\n",
       "        [0.0008]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_init[s[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0118],\n",
       "        [0.0119],\n",
       "        [0.0118],\n",
       "        ...,\n",
       "        [0.0118],\n",
       "        [0.0119],\n",
       "        [0.0118]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y2, y3 = rnn(s[0], None, None)\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8215]], grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_func(y3, s[15])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'loss_fn' object has no attribute 'retain_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-969832296365>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\forpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m    772\u001b[0m             type(self).__name__, name))\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'loss_fn' object has no attribute 'retain_graph'"
     ]
    }
   ],
   "source": [
    "loss_func.retain_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-4c730e92fedd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\forpytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\forpytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "y1, y2, y3 = rnn(s[i], m1, m2)\n",
    "m1 = y1\n",
    "m2 = y2\n",
    "loss = loss_func(y3, s[i+1])\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "i += 1\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7858e-10],\n",
       "        [-4.8181e-10],\n",
       "        [-4.8815e-10],\n",
       "        ...,\n",
       "        [-4.9376e-10],\n",
       "        [-4.8811e-10],\n",
       "        [-4.8762e-10]], requires_grad=True)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0007],\n",
       "        [0.0008],\n",
       "        [0.0008],\n",
       "        ...,\n",
       "        [0.0010],\n",
       "        [0.0006],\n",
       "        [0.0006]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_init['告']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0015, 0.2359],\n",
       "        [0.0047, 0.8872]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#測試\n",
    "y = x.mm(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 4],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[4], [3]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.norm(p=2, dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + x)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[4, 5]], requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros(1, 2, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0, 1] = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
